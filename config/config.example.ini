# ================================================================================
# GMap - Professional Google Maps Scraper & Email Discovery Platform
# ================================================================================
#
# Author: Krishna Kushwaha
# GitHub: https://github.com/krishna-kush
# Project: GMap - Automated Google Maps Scraping + Email Discovery System
# Repository: https://github.com/ScrapeKaBaap/GMap
#
# Description: Enterprise-grade business intelligence platform that combines
#              automated Google Maps scraping with advanced email discovery
#              techniques to build targeted business contact databases.
#
# Components: - Google Maps Company Scraper
#             - Multi-Method Email Discovery (Static, Harvester, Scraper, Checker)
#             - Professional Database Management
#             - Advanced Configuration & Logging System
#
# License: MIT License
# Created: 2025
#
# ================================================================================
# This file is part of the GMap project.
# For complete documentation, visit: https://github.com/ScrapeKaBaap/GMap
# ================================================================================

# ================================================================================
# GMap CONFIGURATION EXAMPLE
# ================================================================================
# This is a comprehensive example configuration file for GMap.
# Copy this file to 'config.ini' and uncomment/modify the settings you need.
#
# IMPORTANT: Only mandatory settings are uncommented in the main config.ini
# All other settings have sensible defaults and can be customized as needed.
#
# Configuration sections:
# - [Playwright]     : Browser automation settings
# - [Search]         : Google Maps search configuration  
# - [Database]       : Database connection settings
# - [EmailFinders]   : Email finding addon configuration
# - [EmailChecker]   : Email validation addon configuration
# - [Logging]        : Logging system configuration
# - [Tests]          : Testing framework settings
# ================================================================================

# ================================================================================
# BROWSER AUTOMATION (Playwright)
# ================================================================================
# Controls how the browser behaves during Google Maps scraping

[Playwright]
# Whether to run browser in headless mode (no visible window)
# Default: False (shows browser window for debugging)
# Set to True for production/server environments
headless = False

# Number of parallel browser tabs/contexts for processing search queries
# Controls how many Google Maps searches run simultaneously
# Example: With 5 tabs, searches "tech companies in Australia", "tech companies in Austria", etc. at same time
# Higher values (8-10) = faster scraping but may trigger Google rate limiting/blocking
# Lower values (1-3) = more stable, less likely to be blocked, but slower
# Recommended: 5 for most users, 1-2 for cautious/slow connections
# Default: 5
# parallel_query_count = 5

# Directory to store browser cookies for session persistence
# Default: cookies
# cookie_dir = cookies

# Directory to store screenshots for debugging
# Default: logs/screenshots  
# screenshot_dir = logs/screenshots

# ================================================================================
# GOOGLE MAPS SEARCH CONFIGURATION
# ================================================================================
# Controls what companies are searched for and where

[Search]
# Template for search queries (${country} and ${state} are replaced automatically)
# Default: tech companies in ${country}
search_query_templates = tech companies in ${country}
# search_query_templates = cloud companies in ${country}, tech companies in ${state}

# Countries to search in (comma-separated)
# Default: Major English-speaking and European countries
country = Australia, Austria, Belgium, Canada, Denmark, Finland, France, Germany, Ireland, Israel, Netherlands, Norway, Singapore, Spain, Sweden, Switzerland, United Kingdom, United States

# States/cities to search in (comma-separated) 
# Default: Major cities worldwide
state = Amsterdam, Antwerp, Barcelona, Berlin, Bern, Brussels, Canberra, Charlotte Amalie, Cockburn Town, Copenhagen, Cork, Douglas, Dublin, Edinburgh, Espoo, Galway, George Town, Ghent, Gibraltar, Hagåtña, Hamilton, Helsinki, Hong Kong, Jerusalem, Kingston, Kuwait City, London, Lyon, Luxembourg City, Macau, Madrid, Majuro, Manama, Marigot, Monaco, Montevideo, Munich, Nassau, Ngerulmud, Nicosia, Nouméa, Nuuk, Oranjestad, Oslo, Ottawa, Oulu, Panama City, Paris, Philipsburg, Port of Spain, Prague, Pretoria, Reykjavik, Road Town, Rome, Saipan, Saint George, Saint John, San Juan, San Marino, Santiago, Seoul, Singapore, Stockholm, Suva, Taipei, Tallinn, Tarawa, Tokyo, Valletta, Vaduz, Vienna, Vilnius, Washington D.C., Wellington, Willemstad, Yaren, Zurich

# Maximum companies to collect per search query
# Default: 100
# max_companies_per_query = 100

# Time to wait after scrolling for new content to load (milliseconds)
# Default: 3000 (3 seconds)
# scroll_wait_time = 3000

# Number of consecutive scrolls with no new companies before ending search
# Default: 3
# max_empty_scrolls = 3

# Number of retry attempts when scrolling fails initially
# Default: 5
# retry_scroll_attempts = 5

# ================================================================================
# DATABASE CONFIGURATION
# ================================================================================
# SQLite database settings for storing scraped company data

[Database]
# Name of the SQLite database file
# Default: google_maps_companies.first.db
db_name = google_maps_companies.first.db

# ================================================================================
# DATABASE COLUMN MAPPING
# ================================================================================
# Configure which database columns to use for company information
# This allows the system to work with different database schemas

# Companies table name
# Default: companies
table_name = companies

# Column names for company information
# These should match the actual column names in your database
# Default: id, name, website
id_column = id
name_column = name
website_column = website

# Optional column containing existing emails (comma-separated)
# If specified, these emails will be included in the final results
# along with newly discovered emails (duplicates are automatically removed)
# Leave empty to disable this feature
# Default: (empty - disabled)
existing_emails_column =

# Example configurations for different database schemas:
# For YC companies database:
# id_column = id
# name_column = company_name
# website_column = website_url
# existing_emails_column = known_emails

# For custom CRM database:
# id_column = company_id
# name_column = business_name
# website_column = domain
# existing_emails_column = contact_emails

# ================================================================================
# EMAIL FINDING ADDONS
# ================================================================================
# Configuration for email discovery methods

[EmailFinders]
# Which email finding methods to enable (comma-separated)
# Available: static, harvester, scraper
# Default: static,harvester,scraper
enabled_methods = static,harvester,scraper

# Whether to run email finding automatically with geo_mail scraping
# Default: true
# Set to false to run email finding separately using addon main.py files
run_inline = true

# Whether to run email validation automatically after finding emails
# Default: false  
# Set to true to validate emails immediately after finding them
check_inline = false

# --------------------------------------------------------------------------------
# STATIC EMAIL GENERATOR SETTINGS
# --------------------------------------------------------------------------------
# Generates common email patterns like info@domain.com, contact@domain.com

# Enable static email generation
# Default: true
static_enabled = true

# Email patterns to generate (comma-separated)
# Available: info, contact, hello, sales, support, help, hr, careers, admin, etc.
# Default: info,contact,hello,sales,support,hr,careers
static_patterns = info,contact,hello,sales,support,hr,careers

# Minimum confidence score for generated emails (0.0 to 1.0)
# Default: 0.7
static_min_confidence = 0.7

# Maximum number of emails to generate per company
# Default: 8
static_max_emails = 8

# Smart pattern selection based on company type and industry
# When enabled, the system analyzes company name/website to choose relevant patterns
# Example: "TechCorp Software" gets tech-focused patterns (api, dev, support)
# Example: "ABC Consulting" gets business-focused patterns (consulting, business, contact)
# When disabled, uses all patterns specified in static_patterns setting
# Default: true
static_smart_selection = true

# Confidence scores for individual patterns (0.0 to 1.0)
# These override the default confidence scores in patterns.py
# Higher confidence = more likely to be a real email address
# Default values shown below:
static_confidence_info = 0.95
static_confidence_contact = 0.90  
static_confidence_hello = 0.85
static_confidence_sales = 0.80
static_confidence_support = 0.80
static_confidence_hr = 0.70
static_confidence_careers = 0.70

# --------------------------------------------------------------------------------
# MAIL HARVESTER SETTINGS (theHarvester integration)
# --------------------------------------------------------------------------------
# Uses theHarvester tool to find emails from search engines and databases

# Enable mail harvester
# Default: true
harvester_enabled = true

# Search engines and databases to use (comma-separated)
# Available: bing, duckduckgo, yahoo, google, crtsh, dnsdumpster, hackertarget, etc.
# Default: bing,duckduckgo,yahoo,crtsh,dnsdumpster,hackertarget
harvester_sources = bing,duckduckgo,yahoo,crtsh,dnsdumpster,hackertarget

# Maximum results per source
# Default: 100
harvester_limit_per_source = 100

# Timeout for harvester operations (seconds)
# Default: 300 (5 minutes)
harvester_timeout = 300

# Confidence score for emails discovered via OSINT/search engines (0.0 to 1.0)
# OSINT emails are found through public databases and search engines
# Lower than scraper confidence since they may be outdated or less reliable
# Default: 0.8
harvester_confidence = 0.8

# Number of concurrent threads for processing multiple companies simultaneously
# Controls how many company domains are harvested at the same time
# Example: With 2 threads, harvests emails for company1.com and company2.com simultaneously
# Higher values (4-8) = faster processing but may trigger rate limiting from search engines
# Lower values (1-2) = more respectful, less likely to be blocked, but slower
# Recommended: 2 for most users, 1 for cautious/slow connections
# Default: 2
harvester_threads = 2

# --------------------------------------------------------------------------------
# MAIL SCRAPER SETTINGS (Website crawling)
# --------------------------------------------------------------------------------
# Crawls company websites to extract email addresses

# Enable mail scraper
# Default: false (requires email_extractor binary)
scraper_enabled = false

# Website crawling depth (1 = homepage only, 2 = homepage + linked pages, -1 = unlimited)
# Default: 1
scraper_depth = 1

# Maximum emails to extract per website
# Default: 20
scraper_limit_emails = 20

# Maximum URLs to crawl per website
# Default: 25
scraper_limit_urls = 25

# Timeout for scraper operations (milliseconds)
# Default: 10000 (10 seconds)
scraper_timeout = 10000

# Sleep delay between individual web requests (milliseconds)
# Controls rate limiting to avoid overwhelming target servers and reduce blocking risk
# Higher values = slower but more respectful scraping, lower values = faster but riskier
# Default: 1000 (1 second between requests)
scraper_sleep = 1000

# Number of concurrent threads for processing multiple companies simultaneously
# Controls how many company websites are scraped at the same time
# Example: With 2 threads, scrapes company1.com and company2.com simultaneously
# Higher values (4-8) = faster processing but may overwhelm target servers or trigger rate limiting
# Lower values (1-2) = more respectful, less likely to be blocked, but slower
# Recommended: 2 for most users, 1 for cautious/slow connections, 3-4 for fast connections
# Default: 2
scraper_threads = 2

# Confidence score for emails found by crawling company websites (0.0 to 1.0)
# Website-scraped emails have highest confidence since they're found on official sites
# Higher than harvester confidence since they're current and directly from source
# Default: 0.9
scraper_confidence = 0.9

# ================================================================================
# EMAIL VALIDATION ADDON
# ================================================================================
# Configuration for email address validation and deliverability checking

[EmailChecker]
# Enable email validation
# Default: true
enabled = true

# API endpoint for email validation service
# Default: http://localhost:8080/v0/check_email
api_endpoint = http://localhost:8080/v0/check_email

# Number of emails to process in each batch
# Default: 200
batch_size = 200

# Maximum concurrent worker threads
# Default: 10
max_workers = 10

# API request timeout (seconds)
# Default: 3600 (1 hour)
api_timeout = 3600

# ================================================================================
# LOGGING CONFIGURATION
# ================================================================================
# Controls logging behavior for the entire application

[Logging]
# Console log level (what appears in terminal)
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
# Default: INFO
console_level = INFO

# File log levels (what gets saved to log files)
# Comma-separated list of levels to save
# Default: DEBUG,INFO,WARNING,ERROR,CRITICAL (saves all levels)
file_levels = DEBUG,INFO,WARNING,ERROR,CRITICAL

# Maximum number of log session folders to keep
# Default: 10
max_log_files_to_keep = 10

# Base directory for log files
# Default: logs
log_dir = logs

# ================================================================================
# TESTING CONFIGURATION
# ================================================================================
# Settings for the testing framework

[Tests]
# Whether to delete test configuration files after tests
# Default: True
delete_test_config = True

# Whether to delete test databases after tests
# Default: False (keep for inspection)
delete_test_db = False

# Name of test configuration file
# Default: test_config.ini
test_config_name = test_config.ini

# Name of test database file
# Default: test_database.db
test_db_name = test_database.db

# ================================================================================
# ADVANCED CONFIGURATION NOTES
# ================================================================================
#
# ADDON ENABLE vs RUN_INLINE LOGIC:
# - enabled=true, run_inline=true   : Addon runs automatically with geo_mail
# - enabled=true, run_inline=false  : Addon available but runs separately
# - enabled=false                   : Addon completely disabled
#
# CONFIDENCE SCORES:
# - Range from 0.0 (no confidence) to 1.0 (very confident)
# - Used to prioritize which emails to try first
# - Higher confidence emails are more likely to be real/working
#
# PERFORMANCE TUNING:
# - Increase parallel_query_count for faster scraping (may trigger rate limits)
# - Increase batch_size and max_workers for faster email processing
# - Adjust timeouts based on your internet connection speed
#
# STANDALONE ADDON USAGE:
# - All addons can be run independently using their main.py files
# - Standalone runs still use this config.ini for settings
# - Example: cd addons/static-generator && python main.py --all-companies
#
# ================================================================================
